{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Logistic Regression with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in the dataset with our processed examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'limited_theorem_classes_2.csv'\n",
    "\n",
    "#Our X data is theorem statements in Lean, while categories are labels\n",
    "categories = []\n",
    "theorems = []\n",
    "\n",
    "\n",
    "with open(file, mode='r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # first row is column labels\n",
    "    for row in csvreader:\n",
    "        categories.append(row[0])\n",
    "        theorems.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Keras' inbuilt tokenizer to map unique words/special characters to unique integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(theorems)\n",
    "sequences = tokenizer.texts_to_sequences(theorems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load in GloVe embeddings from pretrained for us. GloVe is large and isn't submitted to gradescope. Ensure embeddings are downloaded prior to running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove/glove.6B.50d.txt'\n",
    "\n",
    "#Standard load of glove embeddings from given data\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.asarray(values[1:], \"float32\")\n",
    "            embeddings[word] = vec\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each lean statement in our data, we find the respective embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embedding(sequence, embeddings_dict):\n",
    "    vectors = [embeddings_dict.get(token, np.zeros(100)) for token in sequence]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0) #mean pool all word embeddings across lean statement\n",
    "    else:\n",
    "        return np.zeros(100)\n",
    "\n",
    "X_embeddings = np.array([text_to_embedding(sequence, glove_embeddings) for sequence in sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maintain equal vector dimenstions, we pad all lean statements to be the same size as the biggest. This uses Keras' inbuilt function for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode all math categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "categories_index = {category: i for i, category in enumerate(set(categories))}\n",
    "encoded_categories = [categories_index[category] for category in categories]\n",
    "categorical_labels = to_categorical(encoded_categories)\n",
    "print(len(categorical_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77834, 100)\n",
      "(77834, 10)\n",
      "Accuracy: 0.2633134194128605\n"
     ]
    }
   ],
   "source": [
    "y_int_labels = np.argmax(categorical_labels, axis=1) #Convert one-hot into integer from 0-28\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y_int_labels, test_size=0.2, random_state=229) #80-20 split\n",
    "\n",
    "print(X_embeddings.shape)\n",
    "print(categorical_labels.shape)\n",
    "\n",
    "#Use sklearn's multinomial regression\n",
    "model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "result = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
